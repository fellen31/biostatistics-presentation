---
title: "Introduction to biostatistics"
subtitle: "Problem 2"
author: "Felix Lenner, Johnny Pellas & Xue Yu"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
# Set working directory
knitr::opts_knit$set(root.dir = '~/Projects/biostatistics-presentation')
setwd('~/Projects/biostatistics-presentation')

options(htmltools.dir.version = FALSE)

require(tidyverse)
require(broom)
require(ggfortify)

data <- read_tsv("data/problem2_data.txt", col_names = c("Id", "Group", "Week", "Score"), col_types = "nfnn", skip = 1)

data <- data %>%
  mutate(Group = factor(Group, levels = c('Std', 'Exp')),
         Group = recode(Group, 
                        Std = "Standard",
                        Exp = "Experimental (new)"))

data
```

# Presentation of research question 

<!-- "A randomized trial evaluated a new therapy program designed to help patients recover from a stroke, comparing it to a standard program. Patients were evaluated at baseline (week 0), and for 7 consecutive weeks, using the Barthelindex. This score ranges from 0 to 100, higher scores indicating better functional ability". -->

---

# Data description 

<!-- How many individuals are there in the data set?  -->

- Individuals in the data set 

```{r, echo=FALSE}
data %>% distinct(Id) %>% nrow()
```

<!-- Summarize the variables using suitable tables and/or diagrams. -->

- Visualised as boxplots 

```{r, echo=FALSE}
data %>%
  ggplot(aes(factor(Week), Score, fill = Group)) + 
  geom_boxplot() + 
  facet_wrap(~Group) +
  xlab("Week")
```

---

# Individual data

- Why does it make sense to remove individual number 5? 

<!-- Want to look at patients recovering from stroke, if a patient has 100 the patient is already fully (?) recovered at start and should not be included in this trial?  -->

```{r, echo=FALSE}
data %>%
  ggplot(aes(Week, Score, color = factor(Id))) +
  geom_line() +
  facet_wrap(~Group)
```

---

# Data without outlier 

```{r}
data_no_outlier <- data %>% filter(Id !=5)
```

```{r, echo=FALSE}
data_no_outlier %>%
  ggplot(aes(factor(Week), Score, fill = Group)) + 
  geom_boxplot() +
  facet_wrap(~Group) +
  xlab("Week")
```

<!-- "The impression is that both groups start at similar levels (how does this relate to randomization?), and gradually increase, but that the experimental group climbs faster. In a linear regression model, this corresponds to two separate slopes, which can be captured by an interaction term." -->

---

# Statistical analysis

<!-- Which analysis method did you choose?  -->

- We chose linear regression.

<!-- Why? (This should be discussed even if the exam prescribes a certain analysis.)  -->

- "When to prefer regression- If you are interested in the line itself (e.g. the slope), use regression?"

<!-- Explain why you think your chosen analysis is reasonable. -->

---

# Results

<!-- "Analyze the data using multiple linear regression with the following predictors: Group, week, and a group x week interaction term. How to include interaction terms depends on the statistical software." -->

<!-- When presenting your results, tell the audience which number(s) you consider most important, and what they mean. Interpret your results in ordinary language. -->

```{r}
model <- lm(Score ~ Group + Week + Group * Week, data_no_outlier)

summary(model)
```

<!-- You should get 4 estimated regression coefficients, including the intercept. What do they mean? Which one tells us most about the new program? Does it seem promising? -->

- Intercept just means that the median ?? starting point (of standard, why ??) is 37.5 and is significantly different from 0 (?) 

- Does it mean that Week has a positive effect on outcome, and being in the experimental Group boosts this further?

---

# Results continued

- Can be visualized as ? 

```{r}
data_no_outlier %>%
  ggplot(aes(Week, Score, color = Group)) + 
  geom_point(position = position_jitterdodge(), alpha = 0.5) +
  stat_smooth(method = "lm", formula = y ~ x, se = FALSE)

```

---

``` {r}
exp <- data_no_outlier %>%
  filter(Group == "Experimental (new)")

std <- data_no_outlier %>%
  filter(Group == "Standard")

lm(Score ~ Week, exp)
lm(Score ~ Week, std)

exp %>% filter(Week == 0) %>% summarize(median(Score))
std %>% filter(Week == 0) %>% summarize(median(Score))
data_no_outlier %>% filter(Week == 0) %>% summarize(median(Score))
```
---
# Checking model assumptions

``` {r}

# Checking model assumptions
#   - Checking normality
#     - The model assumes the residuals to be normal (not x or y)
#     - After computing these, normality can be checked
#         - Graphically, e.g. histogram, Q-Q plot

augment(model) %>%
  ggplot(aes(.resid, fill = Group)) + 
  geom_histogram(binwidth = 5, position = "identity", alpha = 1) +
  facet_wrap(~Group)

#         - Q-Q plot

autoplot(model)[2]

#         - Numerically, e.g. the Shapiro-Wilk test.

augment(model) %>% pull(.resid) %>% shapiro.test()

```

---
# Bonus question:

> "Bonus question: The assumption of independent measurements is violated in this example. Why? Is this a serious problem? Can you think of an alternative analysis that overcomes this problem? What results does it give?"

- Longitudinal study (several values from the same individual)
- Model assumption of independent measurements is violated, since we have multiple samples (8) corresponding to the different weeks for each individual.

```{r, comment="#"}
# Model assumptions
# "The exact formulation of the linear regression model is:
#   - Linear relationship between x and y
#   - Independent measurements
#   - Normally distributed residuals
#   - Equal SD sigma for the residuals. 
    
#   - The independence assumption
#     - Independence is a question of study design.
#     - The dependent values (y) should be samples individually, not in groups. 
#     - Equivalently: Each row in the database should be independent of other rows (OK to measure x and y on the same person).
#     - Example of non-independence:
#       - Longitudinal studies (several values from the same individual)
#       - Cluster sampling.
#       - Paired data
#   - Dependent data
#     - If the study design violates independence, this _must_ be handled statistically. 
#     - Some possibilities (Chapters 41 - 42):
#       - Combine the dependent values into a single one, e.g. an average or difference. (Like the paired t-test ?)

t_test <- data %>% 
  group_by(Id) %>%
  mutate(diff = Score[which(Week==7)] - Score[which(Week==0)]) %>%
  select(Id, Group, diff) %>%
  distinct()
  

# Maybe ? No longer balanced without number 5
t.test(diff ~ Group, data = t_test, paired = TRUE)

t_test_no_outlier <- data_no_outlier %>% 
  filter(Id != 9) %>% # Remove one at random
  group_by(Id) %>%
  mutate(diff = Score[which(Week==7)] - Score[which(Week==0)]) %>%
  select(Id, Group, diff) %>%
  distinct()
  

t.test(diff ~ Group, data = t_test_no_outlier, paired = TRUE)

data_no_outlier %>% 
  filter(Id != 9) %>% # Remove one at random
  group_by(Id) %>%
  mutate(score_7 = Score[which(Week==7)], score_0 = Score[which(Week==0)]) %>%
  pivot_longer(cols = c("score_7", "score_0"), names_to = "week", values_to = "score") %>%
  select(Id, Group, week, score) %>%
  distinct() %>%
  ggplot(aes(week, score, color = Group, group = Group)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

#       - Use robust standard errors. 
#       - Model the dependence, using a random effects model. 

# Categorical x variables
#   - One often has categorical variables in the data set.
#   - Can we add suck variables to the model? Yes, using dummy variables.
#   - For a dichotomous variable, such as gender, there is only one dummy variable:
#     - B tells how much larger y is for (1) than for (0)

# Smoking example, numerical data in two groups
#   - Could use t-test
#   - Try linear regression with a single variable (smoking) instead!
#   - (This is actually the same thing, as we shall see).

data_no_outlier %>%
  ggplot(aes(group, score)) +
  geom_boxplot()

lm(score ~ group, data_no_outlier)

# Would we like to compare groups for each week? Not really, the important part I guess is the end result (or difference between start and end).

data_no_outlier %>%
  ggplot(aes(week, score, color = group)) +
  geom_jitter(width = 0.2, height = 0, alpha = 0.5)

lm(score ~ group + week, data_no_outlier)

# Interaction
#   - Implicit assumption: The effect of smoking is the same for all ages.
#   - Corresponds to parallel lines.
#   - Couldn't we imagine non-parallel lines, e.g. impaired development for smokers?
#   - Sure! If we wish to study that, we must modify the model, adding an interaction term:
#   - Corresponds to an extras slope for smokers...
#   - When including an interaction, the main effects should also be in the model.

```

---

# Conclusions and discussion

- What results did you get?

- How do you interpret them?

- What biological conclusions do you draw?

- Keep the presentation concise. It should take no longer than 15 minutes, followed by approximately 10 minutes discussion.

---

# Example slide

```{r, fig.width=14,fig.height=5, dev='png', dpi=300}

data %>%
  ggplot(aes(week, score, group = week, fill = group)) + 
  geom_boxplot() +
  facet_wrap(~ group)

```

---

# Example slide

```{r, echo=F, fig.width=14,fig.height=5, dev='png', dpi=300}

data %>%
  ggplot(aes(week, score, group = id, color = factor(id))) +
  geom_line() + 
  facet_wrap(~group)

```

```{r}
diff <- data_no_outlier %>%
  group_by(id) %>%
  mutate(diff = score[which(week==7)] - score[which(week==0)]) %>%
  select(id, group, diff)

diff
```

